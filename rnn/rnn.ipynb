{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129f6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d660599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_shakespeare_text():\n",
    "    path = Path(\"data/shakespeare/shakespeare.txt\")\n",
    "    if not path.is_file():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://homl.info/shakespeare\"\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "    return path.read_text()\n",
    "\n",
    "\n",
    "shakespeare_text = download_shakespeare_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394c8587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\", shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "153dd92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(shakespeare_text.lower()))\n",
    "\"\".join(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10908d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {char: index for index, char in enumerate(vocab)}\n",
    "id_to_char = {index: char for index, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c47e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def encode(t):\n",
    "    return torch.tensor([char_to_id[c] for c in t.lower()])\n",
    "\n",
    "def decode(i):\n",
    "    return \"\".join([id_to_char[id.item()] for id in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28a100be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 17, 24, 24, 27]) hello\n"
     ]
    }
   ],
   "source": [
    "encoded = encode(\"hello\")\n",
    "decoded = decode(encoded)\n",
    "print(encoded, decoded)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "155ead92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self, text, window_length):\n",
    "        self.encode = encode(text)\n",
    "        self.window_length = window_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encode) - self.window_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Dataset Index out of range\")\n",
    "        end = idx + self.window_length\n",
    "        window = self.encode[idx:end]\n",
    "        target = self.encode[idx+1:end+1]\n",
    "        return window, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b52bf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 50\n",
    "batch_size = 512\n",
    "\n",
    "train_set = charDataset(shakespeare_text[:1000000], window_length)\n",
    "valid_set = charDataset(shakespeare_text[1000000:1060000], window_length)\n",
    "test_set = charDataset(shakespeare_text[1060000:], window_length)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc3fcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2674,  0.5349,  0.8094],\n",
       "         [ 2.2082, -0.6380,  0.4617]],\n",
       "\n",
       "        [[ 0.3367,  0.1288,  0.2345],\n",
       "         [ 2.2082, -0.6380,  0.4617]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embed = nn.Embedding(5, 3) #5 categories, 3 dimensions of embeddings\n",
    "embed(torch.tensor([[3, 2], [0, 2]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=18, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embed(x)\n",
    "        x, h = self.gru(embeddings)\n",
    "        x = self.out(x)\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = Model(len(vocab))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d813550b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'h'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m encoded_text \u001b[38;5;241m=\u001b[39m encode(text)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 5\u001b[0m     Y_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     predicted_char_id \u001b[38;5;241m=\u001b[39m Y_logits[\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      7\u001b[0m     predicted_char \u001b[38;5;241m=\u001b[39m id_to_char[predicted_char_id]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'h'"
     ]
    }
   ],
   "source": [
    "model.eval()  # don't forget to switch the model to evaluation mode!\n",
    "text = \"To be or not to b\"\n",
    "encoded_text = encode(text).unsqueeze(dim=0)\n",
    "with torch.no_grad():\n",
    "    Y_logits = model(encoded_text)\n",
    "    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
    "    predicted_char = id_to_char[predicted_char_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c49a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
