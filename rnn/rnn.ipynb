{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129f6bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d660599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_shakespeare_text():\n",
    "    path = Path(\"data/shakespeare/shakespeare.txt\")\n",
    "    if not path.is_file():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://homl.info/shakespeare\"\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "    return path.read_text()\n",
    "\n",
    "\n",
    "shakespeare_text = download_shakespeare_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "394c8587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\", shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "153dd92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(shakespeare_text.lower()))\n",
    "\"\".join(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10908d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {char: index for index, char in enumerate(vocab)}\n",
    "id_to_char = {index: char for index, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c47e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "def encode(t):\n",
    "    return torch.tensor([char_to_id[c] for c in t.lower()])\n",
    "\n",
    "def decode(i):\n",
    "    return \"\".join([id_to_char[id.item()] for id in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28a100be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20, 17, 24, 24, 27]) hello\n"
     ]
    }
   ],
   "source": [
    "encoded = encode(\"hello\")\n",
    "decoded = decode(encoded)\n",
    "print(encoded, decoded)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "155ead92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class charDataset(Dataset):\n",
    "    def __init__(self, text, window_length):\n",
    "        self.encode = encode(text)\n",
    "        self.window_length = window_length\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encode) - self.window_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Dataset Index out of range\")\n",
    "        end = idx + self.window_length\n",
    "        window = self.encode[idx:end]\n",
    "        target = self.encode[idx+1:end+1]\n",
    "        return window, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b52bf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_length = 50\n",
    "batch_size = 512\n",
    "\n",
    "train_set = charDataset(shakespeare_text[:1000000], window_length)\n",
    "valid_set = charDataset(shakespeare_text[1000000:1060000], window_length)\n",
    "test_set = charDataset(shakespeare_text[1060000:], window_length)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc3fcbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2674,  0.5349,  0.8094],\n",
       "         [ 2.2082, -0.6380,  0.4617]],\n",
       "\n",
       "        [[ 0.3367,  0.1288,  0.2345],\n",
       "         [ 2.2082, -0.6380,  0.4617]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "\n",
    "embed = nn.Embedding(5, 3) #5 categories, 3 dimensions of embeddings\n",
    "embed(torch.tensor([[3, 2], [0, 2]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8aa5d132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embed): Embedding(39, 10)\n",
       "  (gru): GRU(10, 18, num_layers=2, batch_first=True, dropout=0.1)\n",
       "  (out): Linear(in_features=18, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=18, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embed(x)\n",
    "        x, h = self.gru(embeddings)\n",
    "        x = self.out(x)\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = Model(len(vocab))\n",
    "model.train()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d813550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # don't forget to switch the model to evaluation mode!\n",
    "text = \"To be or not to b\"\n",
    "encoded_text = encode(text).unsqueeze(dim=0)\n",
    "with torch.no_grad():\n",
    "    Y_logits = model(encoded_text)\n",
    "    predicted_char_id = Y_logits[0, :, -1].argmax().item()\n",
    "    predicted_char = id_to_char[predicted_char_id]\n",
    "    print(predicted_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37c49a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 0, 2, 2, 0, 0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "probs = torch.tensor([[0.5, 0.4, 0.1]])\n",
    "samples = torch.multinomial(probs, num_samples=10, replacement=True)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "db86b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def next_char(model, text, temperature=1):\n",
    "    encoded_text = encode(text).unsqueeze(dim=0)\n",
    "    with torch.no_grad():\n",
    "        Y_logits = model(encoded_text)\n",
    "        Y_probas = F.softmax(Y_logits[0, :, -1] / temperature, dim=-1)\n",
    "        predicted_char_id = torch.multinomial(Y_probas, num_samples=1).item()\n",
    "    return id_to_char[predicted_char_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a088af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(model, text, n_chars=80, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61d00dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to beaaaaaaaaaax aaaaaaaaaaaaaaaaax aaaaaaaaaaaaaaaaaaaaaaaaax saaaaaaaaaaaaaaaaax aa\n"
     ]
    }
   ],
   "source": [
    "print(extend_text(model, \"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eb1a8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
