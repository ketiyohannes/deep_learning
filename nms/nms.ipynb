{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39fd1493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (1.2.1)\n",
      "Requirement already satisfied: packaging in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tokenizers in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from tokenizers) (1.2.1)\n",
      "Requirement already satisfied: filelock in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: shellingham in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.5.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typer-slim in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.13.0)\n",
      "Requirement already satisfied: anyio in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (4.9.0)\n",
      "Requirement already satisfied: certifi in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.0.7)\n",
      "Requirement already satisfied: idna in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (0.14.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from typer-slim->huggingface-hub<2.0,>=0.16.4->tokenizers) (8.1.8)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=0.16.4->tokenizers) (1.3.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install tokenizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e80cc644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06482e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/ketiyohannes/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "nmt_valid, nmt_test = load_dataset(\n",
    "    path = \"ageron/tatoeba_mt_train\", name=\"eng-spa\", split=[\"validation\", \"test\"]\n",
    ")\n",
    "\n",
    "split  = nmt_valid.train_test_split(train_size=0.8, seed=42)\n",
    "train_set, valid_set = split[\"train\"], split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1781a5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_text': 'Tom tried to break up the fight.',\n",
       " 'target_text': 'Tom trató de disolver la pelea.',\n",
       " 'source_lang': 'eng',\n",
       " 'target_lang': 'spa'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "113ec488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tokenizers\n",
    "#lets train a bpe tokenizer\n",
    "def train_eng_spa():\n",
    "    for pair in train_set:\n",
    "        yield pair[\"source_text\"]\n",
    "        yield pair[\"target_text\"]\n",
    "\n",
    "\n",
    "max_length = 256\n",
    "vocab_size =10000\n",
    "\n",
    "tokenizer_model = tokenizers.models.BPE(unk_token=\"<unk>\")\n",
    "\n",
    "tokenizer = tokenizers.Tokenizer(tokenizer_model)\n",
    "tokenizer.enable_padding(pad_id=0, pad_token=\"<pad>\")\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()\n",
    "\n",
    "tokenizer_trainer = tokenizers.trainers.BpeTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(train_eng_spa(), trainer=tokenizer_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ec7e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let’s create a small utility class that will hold tokenized English texts (i.e., the source token ID sequences), along with the corresponding tokenized Spanish targets (i.e., the target token ID sequences), plus the corresponding attention masks. For this, we can create a namedtuple base class (i.e., a tuple with named fields), and extend it to add a to() method, which will make it easy to move all these tensors to the GPU\n",
    "\n",
    "from collections import namedtuple\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "fields = [\"source_ids\", \"source_mask\", \"target_ids\", \"target_mask\"]\n",
    "\n",
    "class NmtPair(namedtuple(\"NmtPairBase\", fields)):\n",
    "    def to(self, device):\n",
    "        return NmtPair(self.source_ids.to(device), self.source_mask.to(device), self.target_ids.to(device), self.target_mask.to(device))\n",
    "\n",
    "\n",
    "def nmt_collate_fn(batch):\n",
    "    src_texts = [pair['source_text'] for pair in batch]\n",
    "    tgt_texts = [f\"<s> {pair['target_text']} </s>\" for pair in batch]\n",
    "    src_encodings = tokenizer.encode_batch(src_texts)\n",
    "    tgt_encodings = tokenizer.encode_batch(tgt_texts)\n",
    "    src_ids = torch.tensor([enc.ids for enc in src_encodings])\n",
    "    tgt_token_ids = torch.tensor([enc.ids for enc in tgt_encodings])\n",
    "    src_mask = torch.tensor([enc.attention_mask for enc in src_encodings])\n",
    "    tgt_mask = torch.tensor([enc.attention_mask for enc in tgt_encodings])\n",
    "\n",
    "    inputs = NmtPair(src_ids, src_mask, tgt_token_ids[:,:-1], tgt_mask[:,:-1])\n",
    "    labels = tgt_token_ids[:, 1:]\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, collate_fn=nmt_collate_fn, shuffle=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, collate_fn=nmt_collate_fn)\n",
    "test_loader = DataLoader(nmt_test, batch_size=batch_size, collate_fn=nmt_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7244b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets build our model\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "class NmtModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, pad_id=0, hidden_dim=512, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.decoder = nn.GRU(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, pair):\n",
    "        src_embed = self.embed(pair.source_ids)\n",
    "        tgt_embed = self.embed(pair.target_ids)\n",
    "        src_lengths = pair.source_mask.sum(dim=1)\n",
    "        src_packed = pack_padded_sequence(src_embed, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, hidden_states = self.encoder(src_packed)\n",
    "        outputs, _ = self.decoder(tgt_embed, hidden_states)\n",
    "        return self.out(outputs).permute(0, 2, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f225f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMT Model with Attention Mechanism (Bahdanau-style)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Bahdanau (additive) attention mechanism\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_dec = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_hidden, source_mask):\n",
    "        # encoder_outputs: (batch, src_len, hidden_dim)\n",
    "        # decoder_hidden: (batch, hidden_dim)\n",
    "        # source_mask: (batch, src_len)\n",
    "        \n",
    "        src_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Project encoder outputs and decoder hidden state\n",
    "        enc_proj = self.W_enc(encoder_outputs)  # (batch, src_len, hidden_dim)\n",
    "        dec_proj = self.W_dec(decoder_hidden).unsqueeze(1)  # (batch, 1, hidden_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        energy = torch.tanh(enc_proj + dec_proj)  # (batch, src_len, hidden_dim)\n",
    "        attention_scores = self.v(energy).squeeze(-1)  # (batch, src_len)\n",
    "        \n",
    "        # Mask out padding positions\n",
    "        attention_scores = attention_scores.masked_fill(source_mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch, src_len)\n",
    "        \n",
    "        # Compute context vector as weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # (batch, 1, hidden_dim)\n",
    "        context = context.squeeze(1)  # (batch, hidden_dim)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "\n",
    "class NmtModelWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=512, pad_id=0, hidden_dim=512, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        # Decoder input now includes context vector concatenated with embedding\n",
    "        self.decoder = nn.GRU(embed_dim + hidden_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim * 2, vocab_size)  # Takes decoder output + context\n",
    "    \n",
    "    def forward(self, pair):\n",
    "        batch_size = pair.source_ids.size(0)\n",
    "        tgt_len = pair.target_ids.size(1)\n",
    "        \n",
    "        # Encode source sequence\n",
    "        src_embed = self.embed(pair.source_ids)\n",
    "        src_lengths = pair.source_mask.sum(dim=1)\n",
    "        src_packed = pack_padded_sequence(src_embed, src_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        encoder_outputs, hidden_states = self.encoder(src_packed)\n",
    "        encoder_outputs, _ = pad_packed_sequence(encoder_outputs, batch_first=True)  # (batch, src_len, hidden_dim)\n",
    "        \n",
    "        # Prepare target embeddings\n",
    "        tgt_embed = self.embed(pair.target_ids)  # (batch, tgt_len, embed_dim)\n",
    "        \n",
    "        # Decode with attention step by step\n",
    "        outputs = []\n",
    "        decoder_hidden = hidden_states\n",
    "        \n",
    "        for t in range(tgt_len):\n",
    "            # Get current target embedding\n",
    "            tgt_t = tgt_embed[:, t:t+1, :]  # (batch, 1, embed_dim)\n",
    "            \n",
    "            # Compute attention using top layer of decoder hidden state\n",
    "            context, attn_weights = self.attention(\n",
    "                encoder_outputs, \n",
    "                decoder_hidden[-1],  # Use top layer hidden state\n",
    "                pair.source_mask\n",
    "            )\n",
    "            \n",
    "            # Concatenate target embedding with context vector\n",
    "            decoder_input = torch.cat([tgt_t, context.unsqueeze(1)], dim=2)  # (batch, 1, embed_dim + hidden_dim)\n",
    "            \n",
    "            # Decoder step\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # Combine decoder output with context for final prediction\n",
    "            combined = torch.cat([decoder_output.squeeze(1), context], dim=1)  # (batch, hidden_dim * 2)\n",
    "            output = self.out(combined)  # (batch, vocab_size)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Stack outputs: (batch, vocab_size, tgt_len) for CrossEntropyLoss\n",
    "        outputs = torch.stack(outputs, dim=2)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "541d0085",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m         mean_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loss, optimizer, train_loader, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m l \u001b[38;5;241m=\u001b[39m loss(y_pred, y)\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m l\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 20\u001b[0m \u001b[43ml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = NmtModel(vocab_size)\n",
    "model = model.to(device)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(model, loss, optimizer, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {mean_loss:.4f}\")\n",
    "\n",
    "\n",
    "train(model, loss, optimizer, train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aacee791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_text, max_length=20, pad_id=0, eos_id=3):\n",
    "    tgt_text = \"\"\n",
    "    token_ids = []\n",
    "    for index in range(max_length):\n",
    "        batch, _ = nmt_collate_fn([{\"source_text\": src_text,\n",
    "                                    \"target_text\": tgt_text}])\n",
    "        with torch.no_grad():\n",
    "            Y_logits = model(batch.to(device))\n",
    "            Y_token_ids = Y_logits.argmax(dim=1)  # find the best token IDs\n",
    "            next_token_id = Y_token_ids[0, index]  # take the last token ID\n",
    "\n",
    "        next_token = tokenizer.id_to_token(next_token_id)\n",
    "        tgt_text += next_token\n",
    "        if next_token_id == eos_id:\n",
    "            break\n",
    "        return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc75efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(translate(model, \"I was eating a pizza\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d6670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe1250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cf92a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
