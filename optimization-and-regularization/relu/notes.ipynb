{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f45457",
   "metadata": {},
   "source": [
    "# A Simple Weight Decay can Improve Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccd2c3",
   "metadata": {},
   "source": [
    "# Weight Decay and Generalization – Full Notebook Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Key Concepts\n",
    "\n",
    "- **Generalization:** The network's ability to perform well on unseen data, not just memorize training examples.\n",
    "- **Linear network:** Output is a weighted sum of inputs:  \n",
    "  $$f(x) = w^T x$$\n",
    "- **Non-linear network:** Networks with activations like ReLU, sigmoid, tanh; output is not a simple sum → introduces curvature.\n",
    "- **Weight vector $w$:** All trainable parameters collected into one vector.\n",
    "- **Hidden layers:** Layers between input and output that transform data.\n",
    "- **Static noise:** Random variation in inputs or targets, usually with zero mean.\n",
    "\n",
    "**Intuition:**  \n",
    "- Linear networks = straight arrows in weight space.  \n",
    "- Non-linear networks = twisted, curved surfaces.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Weight Decay?\n",
    "\n",
    "- **Problem:** Large weights → network can memorize noise → poor generalization.\n",
    "- Noise amplification formula for input noise $\\eta$:  \n",
    "$$\\text{Var}(w^T \\eta) = \\sigma^2 \\|w\\|^2$$  \n",
    "- **Solution:** Add a penalty for large weights:\n",
    "$$E(w) = E_0(w) + \\frac{\\lambda}{2} \\sum_i w_i^2$$  \n",
    "- Gradient descent update:\n",
    "$$\\dot{w}_i = -\\frac{\\partial E_0}{\\partial w_i} - \\lambda w_i$$  \n",
    "\n",
    "**Analogy:**  \n",
    "- Think of weights as a volume knob. Large weights → amplify noise.  \n",
    "- Weight decay → turns down the gain to prevent noise spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feed-Forward Networks\n",
    "\n",
    "- **Network output:** $f_w(e)$  \n",
    "- **Teacher network:** Ideal network $f_u$ with weights $u$  \n",
    "- **Cost function (MSE):**\n",
    "$$E_0(w) = \\frac{1}{2} \\sum_{\\mu=1}^p [f_u(e^\\mu) - f_w(e^\\mu)]^2$$\n",
    "\n",
    "- **Gradient descent with decay:**\n",
    "$$w_i \\leftarrow w_i + \\eta \\sum_{\\mu=1}^{p} [f_u(e^\\mu) - f_w(e^\\mu)] \\frac{\\partial f_w(e^\\mu)}{\\partial w_i} - \\eta \\lambda w_i$$\n",
    "\n",
    "**Explanation:**  \n",
    "- Two forces on weights:\n",
    "  1. **Data force:** reduce error.  \n",
    "  2. **Decay force:** shrink weights → simpler network.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Learning with Noisy Targets\n",
    "\n",
    "- Targets with noise:  \n",
    "$$\\text{Target} = f_u(e) + \\eta, \\quad \\eta \\sim \\text{mean 0, variance } \\sigma^2$$\n",
    "\n",
    "- Weight update:\n",
    "$$\\dot{w}_i \\propto \\sum_{\\mu} \\left( \\frac{1}{N} \\sum_j v_j f_j^\\mu + \\frac{1}{\\sqrt{N}} \\eta^\\mu \\right) f_i^\\mu - \\lambda w_i$$\n",
    "\n",
    "- Asymptotic solution:\n",
    "$$v_r = \\frac{A u_r - \\frac{1}{\\sqrt{N}} \\sum_\\mu \\eta^\\mu f_r^\\mu}{A + A_r}$$\n",
    "\n",
    "- **Optimal weight decay:**\n",
    "$$\\lambda_{\\text{optimal}} = \\frac{\\sigma^2}{|u|^2}$$\n",
    "\n",
    "**Intuition:**  \n",
    "- Noise pushes weights randomly → weight decay acts as a brake.  \n",
    "- Stronger noise → stronger decay needed.  \n",
    "- Analogy: sliders on a soundboard, decay prevents noisy spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Non-Linear Networks\n",
    "\n",
    "- Exact analysis impossible → use **local linearization** (zoom in).  \n",
    "- For realizable functions ($f = f_u$), $p < W$ → **manifold of solutions** (valley) with zero training error.  \n",
    "- Linear expansion:\n",
    "$$\\dot{v}_i \\approx - \\sum_j A_{ij} v_j - \\lambda v_i$$\n",
    "\n",
    "- **Matrix $A$:**  \n",
    "  - Outer product of derivatives → curvature.  \n",
    "  - Rank $R \\leq \\min(p, W)$ → flat directions → valley/rain gutter.  \n",
    "\n",
    "- **Weight decay picks the smallest norm solution** → simpler network, better generalization (Ockham’s Razor).  \n",
    "- Small target errors → same argument as linear case; decay reduces overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c724517",
   "metadata": {},
   "source": [
    "## 6. Visual Intuition (Diagrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8142128",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Valley analogy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Valley analogy\n",
    "w1 = np.linspace(-2,2,100)\n",
    "w2 = np.linspace(-2,2,100)\n",
    "W1, W2 = np.meshgrid(w1, w2)\n",
    "Z = (W1**2 + W2**2) * (1 + 0.5*np.sin(3*W1) * np.sin(3*W2))  # curved valley\n",
    "plt.contourf(W1, W2, Z, levels=50, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.plot(0,0,'ro', label='Smallest norm (weight decay chooses this)')\n",
    "plt.title(\"Weight space valley (curved manifold)\")\n",
    "plt.xlabel(\"Weight w1\")\n",
    "plt.ylabel(\"Weight w2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45611a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: effect of weight decay on linear regression with noisy targets\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data\n",
    "X, y = make_regression(n_samples=50, n_features=1, noise=20, random_state=42)\n",
    "\n",
    "# Fit without weight decay\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "y_pred_no_decay = lr.predict(X)\n",
    "\n",
    "# Fit with weight decay (Ridge regression)\n",
    "ridge = Ridge(alpha=10)  # alpha = lambda\n",
    "ridge.fit(X, y)\n",
    "y_pred_decay = ridge.predict(X)\n",
    "\n",
    "plt.scatter(X, y, label='Data')\n",
    "plt.plot(X, y_pred_no_decay, label='No Decay', color='red')\n",
    "plt.plot(X, y_pred_decay, label='Weight Decay', color='green')\n",
    "plt.legend()\n",
    "plt.title(\"Weight Decay reduces overfitting to noisy data\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
