{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b690b5dc",
   "metadata": {},
   "source": [
    "# ReLU (Rectified Linear Unit) — Key Notes\n",
    "\n",
    "## Definition\n",
    "The **Rectified Linear Unit (ReLU)** is one of the most widely used activation functions in modern neural networks.  \n",
    "It is defined as:\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "- If the input is **positive**, ReLU just passes it through (acts like identity).\n",
    "- If the input is **negative**, ReLU outputs **0** (it \"rectifies\" it).\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative (for backpropagation)\n",
    "ReLU is simple to differentiate:\n",
    "\n",
    "$$f'(x) = \\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "- For positive values → gradient = 1 (no shrinking like sigmoid/tanh).\n",
    "- For negative values → gradient = 0 (inactive neuron).\n",
    "\n",
    "---\n",
    "\n",
    "## Why ReLU is Popular\n",
    "1. **Computationally cheap**  \n",
    "   - Just compares with 0, no exponentials or divisions.\n",
    "   \n",
    "2. **Avoids vanishing gradients** (mostly)  \n",
    "   - Unlike sigmoid/tanh, the gradient for positive inputs is 1, so deep networks can propagate gradients better.\n",
    "\n",
    "3. **Sparsity**  \n",
    "   - Many neurons output exactly 0 → network becomes sparse, which can help generalization and efficiency.\n",
    "\n",
    "4. **Linear behavior for positive inputs**  \n",
    "   - Makes optimization easier compared to bounded activations.\n",
    "\n",
    "---\n",
    "\n",
    "## Drawbacks\n",
    "- **Dying ReLU problem**: Neurons that fall into the negative side can get stuck (always output 0, never update).\n",
    "- Solutions: Leaky ReLU, Parametric ReLU, ELU, GELU, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Properties\n",
    "\n",
    "The ReLU function has these key mathematical characteristics:\n",
    "\n",
    "**Function**: \n",
    "$$f(x) = \\max(0, x) = \\begin{cases}\n",
    "x & \\text{if } x \\geq 0 \\\\\n",
    "0 & \\text{if } x < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "**Derivative**: \n",
    "$$f'(x) = \\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "**Note**: The derivative at $x = 0$ is technically undefined, but in practice we often set $f'(0) = 0$.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- **Formula**: $f(x) = \\max(0, x)$  \n",
    "- **Derivative**: $f'(x) = 1$ (if $x > 0$), else $0$  \n",
    "- **Good for**: Deep networks, faster training, avoiding vanishing gradients  \n",
    "- **Risk**: Some neurons can \"die\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ed7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create input values\n",
    "x = np.linspace(-5, 5, 100)\n",
    "\n",
    "# Apply ReLU function: f(x) = max(0, x)\n",
    "relu_output = np.maximum(0, x)\n",
    "\n",
    "# Plot the function\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot ReLU function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, relu_output, 'b-', linewidth=2, label='ReLU: f(x) = max(0, x)')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel('Output f(x)')\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.legend()\n",
    "\n",
    "# Plot ReLU derivative\n",
    "plt.subplot(1, 2, 2)\n",
    "relu_derivative = np.where(x > 0, 1, 0)\n",
    "plt.plot(x, relu_derivative, 'r-', linewidth=2, label=\"ReLU': f'(x)\")\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)')\n",
    "plt.ylabel(\"Derivative f'(x)\")\n",
    "plt.title('ReLU Derivative')\n",
    "plt.legend()\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate the mathematical properties\n",
    "print(\"ReLU Function Examples:\")\n",
    "test_values = [-2, -1, 0, 1, 2]\n",
    "for val in test_values:\n",
    "    result = max(0, val)\n",
    "    derivative = 1 if val > 0 else 0\n",
    "    print(f\"f({val:2}) = max(0, {val:2}) = {result:2}, f'({val:2}) = {derivative}\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
