{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b690b5dc",
   "metadata": {},
   "source": [
    "# ReLU (Rectified Linear Unit) — Key Notes\n",
    "\n",
    "## Definition\n",
    "The **Rectified Linear Unit (ReLU)** is one of the most widely used activation functions in modern neural networks.  \n",
    "It is defined as:\n",
    "\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition\n",
    "- If the input is **positive**, ReLU just passes it through (acts like identity).\n",
    "- If the input is **negative**, ReLU outputs **0** (it \"rectifies\" it).\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative (for backpropagation)\n",
    "ReLU is simple to differentiate:\n",
    "\n",
    "$$f'(x) = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}$$\n",
    "\n",
    "- For positive values → gradient = 1 (no shrinking like sigmoid/tanh).\n",
    "- For negative values → gradient = 0 (inactive neuron).\n",
    "\n",
    "---\n",
    "\n",
    "## Why ReLU is Popular\n",
    "1. **Computationally cheap**  \n",
    "   - Just compares with 0, no exponentials or divisions.\n",
    "   \n",
    "2. **Avoids vanishing gradients** (mostly)  \n",
    "   - Unlike sigmoid/tanh, the gradient for positive inputs is 1, so deep networks can propagate gradients better.\n",
    "\n",
    "3. **Sparsity**  \n",
    "   - Many neurons output exactly 0 → network becomes sparse, which can help generalization and efficiency.\n",
    "\n",
    "4. **Linear behavior for positive inputs**  \n",
    "   - Makes optimization easier compared to bounded activations.\n",
    "\n",
    "---\n",
    "\n",
    "## Drawbacks\n",
    "- **Dying ReLU problem**: Neurons that fall into the negative side can get stuck (always output 0, never update).\n",
    "- Solutions: Leaky ReLU, Parametric ReLU, ELU, GELU, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- **Formula**: $f(x) = \\max(0, x)$  \n",
    "- **Derivative**: $f'(x) = 1$ (if $x>0$), else $0$.  \n",
    "- **Good for**: Deep networks, faster training, avoiding vanishing gradients.  \n",
    "- **Risk**: Some neurons can \"die\".  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
