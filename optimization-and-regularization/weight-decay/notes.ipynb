{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f45457",
   "metadata": {},
   "source": [
    "# A Simple Weight Decay can Improve Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccd2c3",
   "metadata": {},
   "source": [
    "# Weight Decay and Generalization – Full Notebook Notes\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Key Concepts\n",
    "\n",
    "- **Generalization:** The network's ability to perform well on unseen data, not just memorize training examples.\n",
    "- **Linear network:** Output is a weighted sum of inputs:  \n",
    "  $$f(x) = w^T x$$\n",
    "- **Non-linear network:** Networks with activations like ReLU, sigmoid, tanh; output is not a simple sum → introduces curvature.\n",
    "- **Weight vector $w$:** All trainable parameters collected into one vector.\n",
    "- **Hidden layers:** Layers between input and output that transform data.\n",
    "- **Static noise:** Random variation in inputs or targets, usually with zero mean.\n",
    "\n",
    "**Intuition:**  \n",
    "- Linear networks = straight arrows in weight space.  \n",
    "- Non-linear networks = twisted, curved surfaces.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why Weight Decay?\n",
    "\n",
    "- **Problem:** Large weights → network can memorize noise → poor generalization.\n",
    "- Noise amplification formula for input noise $\\eta$:  \n",
    "$$\\text{Var}(w^T \\eta) = \\sigma^2 \\|w\\|^2$$  \n",
    "- **Solution:** Add a penalty for large weights:\n",
    "$$E(w) = E_0(w) + \\frac{\\lambda}{2} \\sum_i w_i^2$$  \n",
    "- Gradient descent update:\n",
    "$$\\dot{w}_i = -\\frac{\\partial E_0}{\\partial w_i} - \\lambda w_i$$  \n",
    "\n",
    "**Analogy:**  \n",
    "- Think of weights as a volume knob. Large weights → amplify noise.  \n",
    "- Weight decay → turns down the gain to prevent noise spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Feed-Forward Networks\n",
    "\n",
    "- **Network output:** $f_w(e)$  \n",
    "- **Teacher network:** Ideal network $f_u$ with weights $u$  \n",
    "- **Cost function (MSE):**\n",
    "$$E_0(w) = \\frac{1}{2} \\sum_{\\mu=1}^p [f_u(e^\\mu) - f_w(e^\\mu)]^2$$\n",
    "\n",
    "- **Gradient descent with decay:**\n",
    "$$w_i \\leftarrow w_i + \\eta \\sum_{\\mu=1}^{p} [f_u(e^\\mu) - f_w(e^\\mu)] \\frac{\\partial f_w(e^\\mu)}{\\partial w_i} - \\eta \\lambda w_i$$\n",
    "\n",
    "**Explanation:**  \n",
    "- Two forces on weights:\n",
    "  1. **Data force:** reduce error.  \n",
    "  2. **Decay force:** shrink weights → simpler network.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Learning with Noisy Targets\n",
    "\n",
    "- Targets with noise:  \n",
    "$$\\text{Target} = f_u(e) + \\eta, \\quad \\eta \\sim \\text{mean 0, variance } \\sigma^2$$\n",
    "\n",
    "- Weight update:\n",
    "$$\\dot{w}_i \\propto \\sum_{\\mu} \\left( \\frac{1}{N} \\sum_j v_j f_j^\\mu + \\frac{1}{\\sqrt{N}} \\eta^\\mu \\right) f_i^\\mu - \\lambda w_i$$\n",
    "\n",
    "- Asymptotic solution:\n",
    "$$v_r = \\frac{A u_r - \\frac{1}{\\sqrt{N}} \\sum_\\mu \\eta^\\mu f_r^\\mu}{A + A_r}$$\n",
    "\n",
    "- **Optimal weight decay:**\n",
    "$$\\lambda_{\\text{optimal}} = \\frac{\\sigma^2}{|u|^2}$$\n",
    "\n",
    "**Intuition:**  \n",
    "- Noise pushes weights randomly → weight decay acts as a brake.  \n",
    "- Stronger noise → stronger decay needed.  \n",
    "- Analogy: sliders on a soundboard, decay prevents noisy spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Non-Linear Networks\n",
    "\n",
    "- Exact analysis impossible → use **local linearization** (zoom in).  \n",
    "- For realizable functions ($f = f_u$), $p < W$ → **manifold of solutions** (valley) with zero training error.  \n",
    "- Linear expansion:\n",
    "$$\\dot{v}_i \\approx - \\sum_j A_{ij} v_j - \\lambda v_i$$\n",
    "\n",
    "- **Matrix $A$:**  \n",
    "  - Outer product of derivatives → curvature.  \n",
    "  - Rank $R \\leq \\min(p, W)$ → flat directions → valley/rain gutter.  \n",
    "\n",
    "- **Weight decay picks the smallest norm solution** → simpler network, better generalization (Ockham’s Razor).  \n",
    "- Small target errors → same argument as linear case; decay reduces overfitting.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
