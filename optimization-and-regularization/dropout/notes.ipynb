{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601073e3",
   "metadata": {},
   "source": [
    "# Dropout Neural Networks – Notes\n",
    "\n",
    "This section summarizes the **dropout model** and how it is trained.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Standard Feedforward Network\n",
    "\n",
    "Consider a neural network with $L$ hidden layers:\n",
    "\n",
    "- $l \\in \\{1, \\dots, L\\}$ indexes the hidden layers.  \n",
    "- $z^{(l)}$ = vector of inputs to layer $l$  \n",
    "- $y^{(l)}$ = vector of outputs from layer $l$ (with $y^{(0)} = x$, the input)  \n",
    "- $W^{(l)}, b^{(l)}$ = weights and biases of layer $l$\n",
    "\n",
    "The standard feedforward operation for hidden unit $i$ is:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "z_i^{(l+1)} &= W_i^{(l+1)} y^{(l)} + b_i^{(l+1)} \\\\\n",
    "y_i^{(l+1)} &= f(z_i^{(l+1)})\n",
    "\\end{aligned}$$\n",
    "\n",
    "where $f$ is an activation function, e.g., sigmoid:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Feedforward with Dropout\n",
    "\n",
    "Dropout modifies the forward pass by **randomly dropping units**:\n",
    "\n",
    "$$r_j^{(l)} \\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "$$\\tilde{y}^{(l)} = r^{(l)} \\odot y^{(l)}$$\n",
    "\n",
    "$$z_i^{(l+1)} = W_i^{(l+1)} \\tilde{y}^{(l)} + b_i^{(l+1)}$$\n",
    "\n",
    "$$y_i^{(l+1)} = f(z_i^{(l+1)})$$\n",
    "\n",
    "- $\\odot$ = element-wise product  \n",
    "- $r^{(l)}$ = vector of independent Bernoulli random variables with probability $p$ of being 1  \n",
    "- $\\tilde{y}^{(l)}$ = **thinned outputs** used as input to the next layer  \n",
    "- At **test time**, scale weights: $W^{(l)}_{\\text{test}} = p W^{(l)}$  \n",
    "\n",
    "> Intuition: Dropout samples a **sub-network** from the full network. Training over many random sub-networks approximates averaging predictions over an exponential number of models.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training Dropout Nets\n",
    "\n",
    "### 3.1 Backpropagation\n",
    "\n",
    "- Use **stochastic gradient descent (SGD)** similar to standard networks.  \n",
    "- For each training example:\n",
    "  1. Sample a **thinned network** by dropping units.  \n",
    "  2. Perform **forward and backward pass** on this sub-network.  \n",
    "- Gradients for each parameter are **averaged over the mini-batch**.  \n",
    "  - If a parameter isn’t used in a training case → gradient = 0.  \n",
    "\n",
    "**Additional techniques that help:**\n",
    "\n",
    "- Momentum  \n",
    "- Annealed learning rates  \n",
    "- L2 weight decay  \n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Max-norm Regularization\n",
    "\n",
    "- Constrain incoming weight vector of each hidden unit:\n",
    "\n",
    "$$\\| w \\|_2 \\leq c$$\n",
    "\n",
    "- If $w$ goes out of this ball → **project back** onto the ball of radius $c$.  \n",
    "- Helps prevent weights from **blowing up**, especially with **high learning rates**.  \n",
    "- Often used **together with dropout**, large decaying learning rates, and high momentum for better performance.  \n",
    "\n",
    "> Noise from dropout allows the optimizer to explore **different regions of weight space**, improving generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Unsupervised Pretraining\n",
    "\n",
    "- Networks can be pretrained using:\n",
    "  - RBMs (Restricted Boltzmann Machines)  \n",
    "  - Autoencoders  \n",
    "  - Deep Boltzmann Machines  \n",
    "\n",
    "- Pretraining uses **unlabeled data** to initialize weights.  \n",
    "- **Dropout finetuning** procedure:\n",
    "  1. Scale pretrained weights by $1/p$  \n",
    "  2. Use smaller learning rates than random initialization finetuning  \n",
    "- Ensures **pretrained information is retained**, while still benefiting from dropout’s regularization.  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Key Takeaways\n",
    "\n",
    "1. Dropout approximates **Bayesian model averaging** by sampling sub-networks.  \n",
    "2. Training requires **randomly masking units** per example, averaging gradients across mini-batch.  \n",
    "3. Max-norm regularization + dropout + careful learning rates → **best generalization performance**.  \n",
    "4. Works well even when combined with **pretraining** on unlabeled data.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Optional Diagram Idea:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Practical Implementation Example\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def apply_dropout(layer_output, dropout_prob, training=True):\n",
    "    \"\"\"\n",
    "    Apply dropout to a layer's output\n",
    "    \n",
    "    Args:\n",
    "        layer_output: numpy array of shape (batch_size, num_units)\n",
    "        dropout_prob: probability of keeping each unit (p in the paper)\n",
    "        training: whether we're in training mode\n",
    "    \n",
    "    Returns:\n",
    "        Thinned output if training, scaled output if testing\n",
    "    \"\"\"\n",
    "    if training:\n",
    "        # Generate Bernoulli random variables: r^(l) ~ Bernoulli(p)\n",
    "        mask = np.random.binomial(1, dropout_prob, size=layer_output.shape)\n",
    "        # Element-wise product: ỹ^(l) = r^(l) ⊙ y^(l)\n",
    "        return layer_output * mask\n",
    "    else:\n",
    "        # At test time, scale by dropout probability\n",
    "        return layer_output * dropout_prob\n",
    "\n",
    "# Demonstrate dropout effect\n",
    "np.random.seed(42)\n",
    "layer_size = 10\n",
    "batch_size = 5\n",
    "dropout_prob = 0.5\n",
    "\n",
    "# Simulate layer outputs\n",
    "y = np.random.randn(batch_size, layer_size)\n",
    "print(\"Original layer outputs:\")\n",
    "print(y.round(2))\n",
    "\n",
    "print(f\"\\nWith dropout (p={dropout_prob}, training=True):\")\n",
    "y_dropout = apply_dropout(y, dropout_prob, training=True)\n",
    "print(y_dropout.round(2))\n",
    "print(f\"Units kept: {np.sum(y_dropout != 0)}/{y_dropout.size}\")\n",
    "\n",
    "print(f\"\\nAt test time (p={dropout_prob}, training=False):\")\n",
    "y_test = apply_dropout(y, dropout_prob, training=False)\n",
    "print(y_test.round(2))\n",
    "\n",
    "# Visualize dropout effect\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "im1 = axes[0].imshow(y, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('Original Layer Output')\n",
    "axes[0].set_xlabel('Units')\n",
    "axes[0].set_ylabel('Batch Examples')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Training (with dropout)\n",
    "im2 = axes[1].imshow(y_dropout, cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title(f'Training: With Dropout (p={dropout_prob})')\n",
    "axes[1].set_xlabel('Units')\n",
    "axes[1].set_ylabel('Batch Examples')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Test time (scaled)\n",
    "im3 = axes[2].imshow(y_test, cmap='RdBu', aspect='auto')\n",
    "axes[2].set_title(f'Test: Scaled by p={dropout_prob}')\n",
    "axes[2].set_xlabel('Units')\n",
    "axes[2].set_ylabel('Batch Examples')\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mathematical relationship demonstration\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MATHEMATICAL RELATIONSHIPS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Show the mathematical formulation\n",
    "print(\"1. Dropout mask generation:\")\n",
    "print(\"   r_j^(l) ~ Bernoulli(p)\")\n",
    "print(f\"   Example mask: {(y_dropout != 0).astype(int)[0]}\")\n",
    "\n",
    "print(\"\\n2. Thinned outputs (training):\")\n",
    "print(\"   ỹ^(l) = r^(l) ⊙ y^(l)\")\n",
    "print(f\"   Original: {y[0].round(2)}\")\n",
    "print(f\"   Thinned:  {y_dropout[0].round(2)}\")\n",
    "\n",
    "print(\"\\n3. Test time scaling:\")\n",
    "print(\"   W_test^(l) = p * W^(l)\")\n",
    "print(f\"   Scaled output: {y_test[0].round(2)}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17fe2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
