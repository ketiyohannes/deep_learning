{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b7bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def download_dataset(): \n",
    "    path = Path(\"data/shakespeare/shakespeare.txt\")\n",
    "    if not path.is_file():\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://homl.info/shakespeare\"\n",
    "        urllib.request.urlretrieve(url, path)\n",
    "    return path.read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8213f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "shakespeare_text = download_dataset()\n",
    "print(shakespeare_text[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc10d29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0105f834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "39\n",
      ":\n",
      "\n",
      " !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "# we need to convert the text into a sequence of characters then convert each character into a unique integer\n",
    "vocab = sorted(set(shakespeare_text.lower())) #created a list of sorted unique non duplicate characters that are found in the text\n",
    "print(\"\".join(vocab)) #printing all the characters in the vobaulary removing duplicates and spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40eda07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we assing token id to each charater, we'll use the index of each character in vocab as the token id\n",
    "char_to_id = {char: index for index, char in enumerate(vocab)} # the key is the character and the value is the index\n",
    "id_to_char = {index: char for index, char in enumerate(vocab)} # the key is the index and the value is the character\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a2014ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create functions to encode and decode texts\n",
    "\n",
    "import torch\n",
    "\n",
    "def encode_text(text: str):\n",
    "    return torch.tensor([char_to_id[char] for char in text.lower()])\n",
    "\n",
    "def decode_text(ids: torch.Tensor):\n",
    "    return \"\".join(id_to_char[id] for id in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# defining our datasets by creating a custom dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text: str, window_length: int):\n",
    "        self.text = encode_text(text)\n",
    "        self.window_length = window_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.window_length\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if index >= len(self):\n",
    "            raise IndexError(\"dataset index out of range\")\n",
    "        end = index + self.window_length\n",
    "        window = self.text[index: end]\n",
    "        target = self.text[index + 1: end + 1]\n",
    "        return window, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96a6e058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configs\n",
    "window_length = 50\n",
    "batch_size = 512\n",
    "\n",
    "#creating our datasets, train test and validation\n",
    "trainset = CharDataset(shakespeare_text[:1000000], window_length)\n",
    "validset = CharDataset(shakespeare_text[1000000:1060000], window_length)\n",
    "testset = CharDataset(shakespeare_text[1060000:], window_length)\n",
    "\n",
    "#creating our dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(validset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae4ebbca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2674,  0.5349,  0.8094],\n",
       "         [ 2.2082, -0.6380,  0.4617]],\n",
       "\n",
       "        [[ 0.3367,  0.1288,  0.2345],\n",
       "         [ 2.2082, -0.6380,  0.4617]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets create some embeddings, lets initialize an embedding layer with 5 characters and 3 dimensions\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "embed = nn.Embedding(5,3)\n",
    "embed(torch.tensor([[3, 2], [0, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cb616e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets build our shakespeare char rnn model\n",
    "\n",
    "class ShakespeareModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_layers=2, embed_dim=10, hidden_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.embed(x)\n",
    "        outputs, _states = self.gru(embeddings)\n",
    "        return self.output(outputs).permute(0, 2, 1)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = ShakespeareModel(len(vocab)).to(device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d871c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3.0872\n",
      "Epoch 2/10, Loss: 3.0409\n",
      "Epoch 3/10, Loss: 2.9758\n",
      "Epoch 4/10, Loss: 2.8775\n",
      "Epoch 5/10, Loss: 2.7620\n",
      "Epoch 6/10, Loss: 2.6717\n",
      "Epoch 7/10, Loss: 2.6064\n",
      "Epoch 8/10, Loss: 2.5508\n",
      "Epoch 9/10, Loss: 2.5026\n",
      "Epoch 10/10, Loss: 2.4601\n"
     ]
    }
   ],
   "source": [
    "#lets create a loss function and optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "def train(model, loss, optimizer, train_loader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = model(x)\n",
    "            l = loss(y_pred, y)\n",
    "            total_loss += l.item()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        mean_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {mean_loss:.4f}\")\n",
    "\n",
    "\n",
    "train(model, loss, optimizer, trainloader, epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be that is the question\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "text = \"To be or not to be that is the questio\"\n",
    "encoded = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_logits = model(encoded)\n",
    "    pred_char_id = y_logits[0, :, -1].argmax().item()\n",
    "    pred_char = id_to_char[pred_char_id]\n",
    "\n",
    "print(text + pred_char)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10e964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be\n",
      "To be or not to be \n",
      "To be or not to be t\n",
      "To be or not to be th\n",
      "To be or not to be the\n",
      "To be or not to be the \n",
      "To be or not to be the t\n",
      "To be or not to be the th\n",
      "To be or not to be the the\n",
      "To be or not to be the the \n"
     ]
    }
   ],
   "source": [
    "text = \"To be or not to b\"\n",
    "i=0\n",
    "while i < 10:\n",
    "    encoded = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_logits = model(encoded)\n",
    "        pred_char_id = y_logits[0, :, -1].argmax().item()\n",
    "        pred_char = id_to_char[pred_char_id]\n",
    "        text += pred_char\n",
    "        print(text)\n",
    "        i += 1\n",
    "\n",
    "#this is what we call greedy decoding, we always take the most likely character at each step. it results in the same character being repeated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7001c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 1, 0, 2, 2, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "#lets use model estimation probaility sampling using multinomial distribution\n",
    "#lets look at an example\n",
    "torch.manual_seed(42)\n",
    "p = torch.tensor([[0.5, 0.4, 0.1]]) # we are saying the probs of the 3 characters are 50%, 40% and 10%\n",
    "samples = torch.multinomial(p, num_samples=10, replacement=True) # we are sampling 10 times from the distribution\n",
    "print(samples)\n",
    "\n",
    "#we will also use temperature to control the randomness of the sampling\n",
    "#temperature is a hyperparameter that controls the randomness of the sampling\n",
    "#if temperature is high, the model will be more random\n",
    "#if temperature is low, the model will be more deterministic\n",
    "#we will use temperature to sample from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57849b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to by i lith\n",
      "thar yome your mey thoo pis the couf,\n",
      "ad ton was tith for pou couly\n",
      "sad\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def next_char(model, text, temperature=1.0):\n",
    "    encoded = encode_text(text).unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        y_logits = model(encoded)\n",
    "        y_prob = F.softmax(y_logits[0, :, -1] / temperature, dim=-1)\n",
    "        predicted_char_id = torch.multinomial(y_prob, num_samples=1).item()\n",
    "        return id_to_char[predicted_char_id]\n",
    "\n",
    "def extend_text(model, text, num_chars=80, temperature=1.0):\n",
    "    model.eval()\n",
    "    for i in range(num_chars):\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text\n",
    "\n",
    "print(extend_text(model, \"to be or not to b\", 80, temperature=0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#top k and beam search as an alternative to this type of p sampling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
